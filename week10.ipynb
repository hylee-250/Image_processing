{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8453f073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import huffman\n",
    "from dahuffman import HuffmanCodec\n",
    "\n",
    "f = open(\"C:\\\\Work\\\\Image_processing\\\\text.txt\",\"r\", encoding='UTF-8')\n",
    "text = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "176b762b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Recently', 'neural', 'networks', 'purely', 'based', 'on', 'attention', 'were', 'shown', 'to', 'address', 'image', 'understanding', 'tasks', 'such', 'as', 'classification', 'These', 'high', 'performing', 'vision', 'transformers', 'are', 'pretrained', 'with', 'hundreds', 'of', 'millions', 'images', 'using', 'a', 'large', 'infrastructure', 'thereby', 'limiting', 'their', 'adoption', 'In', 'this', 'work', 'we', 'produce', 'competitive', 'convolutionfree', 'by', 'training', 'Imagenet', 'only', 'We', 'train', 'them', 'single', 'computer', 'in', 'less', 'than', '3', 'days', 'Our', 'reference', 'transformer', '86M', 'parameters', 'achieves', 'top1', 'accuracy', '831', 'singlecrop', 'ImageNet', 'no', 'external', 'data', 'More', 'importantly', 'introduce', 'teacherstudent', 'strategy', 'specific', 'It', 'relies', 'distillation', 'token', 'ensuring', 'that', 'the', 'student', 'learns', 'from', 'teacher', 'through', 'show', 'interest', 'tokenbased', 'especially', 'when', 'convnet', 'This', 'leads', 'us', 'report', 'results', 'convnets', 'for', 'both', 'where', 'obtain', 'up', '852', 'and', 'transferring', 'other', 'share', 'our', 'code', 'models', '1', 'Introduction', 'Convolutional', 'have', 'been', 'main', 'design', 'paradigm', 'initially', 'demonstrated', 'One', 'ingredient', 'success', 'was', 'availability', 'set', 'namely', '13', '42', 'Motivated', 'attentionbased', 'Natural', 'Language', 'Processing', '14', '52', 'there', 'has', 'increasing', 'architectures', 'leveraging', 'mechanisms', 'within', '2', '34', '61', 'recently', 'several', 'researchers', 'proposed', 'hybrid', 'architecture', 'transplanting', 'ingredients', 'solve', '6', '43', 'The', 'ViT', 'introduced', 'Dosovitskiy', 'et', 'al', '15', 'is', 'an', 'directly', 'inherited', '', 'but', 'applied', 'raw', 'patches', 'input', 'Their', 'paper', 'presented', 'excellent', 'trained', 'private', 'labelled', 'dataset', 'JFT300M', '46', '300', 'concluded', 'do', 'not', 'generalize', 'well', 'insufficient', 'amounts', 'these', 'involved', 'extensive', 'computing', 'resources', '8GPU', 'node', 'two', 'three', '53', 'hours', 'pretraining', 'optionally', '20', 'finetuning', 'having', 'similar', 'number', 'efficiency', 'uses', 'sole', 'build', 'upon', 'visual', 'improvements', 'included', 'timm', 'library', '55', 'With', 'Dataefficient', 'Transformers', 'DeiT', 'over', 'previous', 'see', 'Figure', 'ablation', 'study', 'details', 'hyperparameters', 'key', 'successful', 'repeated', 'augmentation', 'another', 'question', 'how', 'distill', 'denoted', 'it', 'advantageously', 'replaces', 'usual', 'summary', 'makes', 'following', 'contributions', 'contains', 'convolutional', 'layer', 'can', 'achieve', 'against', 'state', 'art', 'They', 'learned', '4', 'GPUs', 'days1', 'new', 'DeiTS', 'DeiTTi', 'fewer', 'be', 'seen', 'counterpart', 'ResNet50', 'ResNet18', 'procedure', 'which', 'plays', 'same', 'role', 'class', 'except', 'aims', 'at', 'reproducing', 'label', 'estimated', 'Both', 'tokens', 'interact', 'transformerspecific', 'outperforms', 'vanilla', 'significant', 'margin', 'Interestingly', 'learn', 'more', 'comparable', 'performance', 'prelearned', 'transferred', 'different', 'downstream', 'finegrained', 'popular', 'public', 'benchmarks', 'CIFAR10', 'CIFAR100', 'Oxford102', 'flowers', 'Stanford', 'Cars', 'iNaturalist1819', 'organized', 'follows', 'review', 'related', 'works', 'Section', 'focus', 'experimental', 'section', '5', 'provides', 'analysis', 'comparisons', 'recent', 'comparative', 'evaluation', 'scheme', 'includes', 'dataefficient', 'choices', 'gives', 'some', 'insight', 'conclude', '7']\n"
     ]
    }
   ],
   "source": [
    "c_list = text.replace('\\n',' ').split(\" \")\n",
    "symbol =[]\n",
    "for idx in range(len(c_list)):\n",
    "    c_list[idx] = re.sub(r\"[^a-zA-Z0-9]\",\"\",c_list[idx])\n",
    "for string in c_list:\n",
    "    if string not in symbol:\n",
    "        symbol.append(string)\n",
    "print(symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b41286ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(342,)\n"
     ]
    }
   ],
   "source": [
    "count = np.zeros(len(symbol)).astype(np.uint16)\n",
    "print(count.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "08f17d76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Recently', 'neural', 'networks', 'purely', 'based', 'on', 'attention', 'were', 'shown', 'to', 'address', 'image', 'understanding', 'tasks', 'such', 'as', 'image', 'classification', 'These', 'high', 'performing', 'vision', 'transformers', 'are', 'pretrained', 'with', 'hundreds', 'of', 'millions', 'of', 'images', 'using', 'a', 'large', 'infrastructure', 'thereby', 'limiting', 'their', 'adoption', 'In', 'this', 'work', 'we', 'produce', 'competitive', 'convolutionfree', 'transformers', 'by', 'training', 'on', 'Imagenet', 'only', 'We', 'train', 'them', 'on', 'a', 'single', 'computer', 'in', 'less', 'than', '3', 'days', 'Our', 'reference', 'vision', 'transformer', '86M', 'parameters', 'achieves', 'top1', 'accuracy', 'of', '831', 'singlecrop', 'on', 'ImageNet', 'with', 'no', 'external', 'data', 'More', 'importantly', 'we', 'introduce', 'a', 'teacherstudent', 'strategy', 'specific', 'to', 'transformers', 'It', 'relies', 'on', 'a', 'distillation', 'token', 'ensuring', 'that', 'the', 'student', 'learns', 'from', 'the', 'teacher', 'through', 'attention', 'We', 'show', 'the', 'interest', 'of', 'this', 'tokenbased', 'distillation', 'especially', 'when', 'using', 'a', 'convnet', 'as', 'a', 'teacher', 'This', 'leads', 'us', 'to', 'report', 'results', 'competitive', 'with', 'convnets', 'for', 'both', 'Imagenet', 'where', 'we', 'obtain', 'up', 'to', '852', 'accuracy', 'and', 'when', 'transferring', 'to', 'other', 'tasks', 'We', 'share', 'our', 'code', 'and', 'models', '1', 'Introduction', 'Convolutional', 'neural', 'networks', 'have', 'been', 'the', 'main', 'design', 'paradigm', 'for', 'image', 'understanding', 'tasks', 'as', 'initially', 'demonstrated', 'on', 'image', 'classification', 'tasks', 'One', 'of', 'the', 'ingredient', 'to', 'their', 'success', 'was', 'the', 'availability', 'of', 'a', 'large', 'training', 'set', 'namely', 'Imagenet', '13', '42', 'Motivated', 'by', 'the', 'success', 'of', 'attentionbased', 'models', 'in', 'Natural', 'Language', 'Processing', '14', '52', 'there', 'has', 'been', 'increasing', 'interest', 'in', 'architectures', 'leveraging', 'attention', 'mechanisms', 'within', 'convnets', '2', '34', '61', 'More', 'recently', 'several', 'researchers', 'have', 'proposed', 'hybrid', 'architecture', 'transplanting', 'transformer', 'ingredients', 'to', 'convnets', 'to', 'solve', 'vision', 'tasks', '6', '43', 'The', 'vision', 'transformer', 'ViT', 'introduced', 'by', 'Dosovitskiy', 'et', 'al', '15', 'is', 'an', 'architecture', 'directly', 'inherited', 'from', 'Natural', 'Language', 'Processing', '52', '', 'but', 'applied', 'to', 'image', 'classification', 'with', 'raw', 'image', 'patches', 'as', 'input', 'Their', 'paper', 'presented', 'excellent', 'results', 'with', 'transformers', 'trained', 'with', 'a', 'large', 'private', 'labelled', 'image', 'dataset', 'JFT300M', '46', '300', 'millions', 'images', 'The', 'paper', 'concluded', 'that', 'transformers', 'do', 'not', 'generalize', 'well', 'when', 'trained', 'on', 'insufficient', 'amounts', 'of', 'data', 'and', 'the', 'training', 'of', 'these', 'models', 'involved', 'extensive', 'computing', 'resources', 'In', 'this', 'paper', 'we', 'train', 'a', 'vision', 'transformer', 'on', 'a', 'single', '8GPU', 'node', 'in', 'two', 'to', 'three', 'days', '53', 'hours', 'of', 'pretraining', 'and', 'optionally', '20', 'hours', 'of', 'finetuning', 'that', 'is', 'competitive', 'with', 'convnets', 'having', 'a', 'similar', 'number', 'of', 'parameters', 'and', 'efficiency', 'It', 'uses', 'Imagenet', 'as', 'the', 'sole', 'training', 'set', 'We', 'build', 'upon', 'the', 'visual', 'transformer', 'architecture', 'from', 'Dosovitskiy', 'et', 'al', '15', 'and', 'improvements', 'included', 'in', 'the', 'timm', 'library', '55', 'With', 'our', 'Dataefficient', 'image', 'Transformers', 'DeiT', 'we', 'report', 'large', 'improvements', 'over', 'previous', 'results', 'see', 'Figure', '1', 'Our', 'ablation', 'study', 'details', 'the', 'hyperparameters', 'and', 'key', 'ingredients', 'for', 'a', 'successful', 'training', 'such', 'as', 'repeated', 'augmentation', 'We', 'address', 'another', 'question', 'how', 'to', 'distill', 'these', 'models', 'We', 'introduce', 'a', 'tokenbased', 'strategy', 'specific', 'to', 'transformers', 'and', 'denoted', 'by', 'DeiT', 'and', 'show', 'that', 'it', 'advantageously', 'replaces', 'the', 'usual', 'distillation', 'In', 'summary', 'our', 'work', 'makes', 'the', 'following', 'contributions', '', 'We', 'show', 'that', 'our', 'neural', 'networks', 'that', 'contains', 'no', 'convolutional', 'layer', 'can', 'achieve', 'competitive', 'results', 'against', 'the', 'state', 'of', 'the', 'art', 'on', 'ImageNet', 'with', 'no', 'external', 'data', 'They', 'are', 'learned', 'on', 'a', 'single', 'node', 'with', '4', 'GPUs', 'in', 'three', 'days1', 'Our', 'two', 'new', 'models', 'DeiTS', 'and', 'DeiTTi', 'have', 'fewer', 'parameters', 'and', 'can', 'be', 'seen', 'as', 'the', 'counterpart', 'of', 'ResNet50', 'and', 'ResNet18', '', 'We', 'introduce', 'a', 'new', 'distillation', 'procedure', 'based', 'on', 'a', 'distillation', 'token', 'which', 'plays', 'the', 'same', 'role', 'as', 'the', 'class', 'token', 'except', 'that', 'it', 'aims', 'at', 'reproducing', 'the', 'label', 'estimated', 'by', 'the', 'teacher', 'Both', 'tokens', 'interact', 'in', 'the', 'transformer', 'through', 'attention', 'This', 'transformerspecific', 'strategy', 'outperforms', 'vanilla', 'distillation', 'by', 'a', 'significant', 'margin', '', 'Interestingly', 'with', 'our', 'distillation', 'image', 'transformers', 'learn', 'more', 'from', 'a', 'convnet', 'than', 'from', 'another', 'transformer', 'with', 'comparable', 'performance', '', 'Our', 'models', 'prelearned', 'on', 'Imagenet', 'are', 'competitive', 'when', 'transferred', 'to', 'different', 'downstream', 'tasks', 'such', 'as', 'finegrained', 'classification', 'on', 'several', 'popular', 'public', 'benchmarks', 'CIFAR10', 'CIFAR100', 'Oxford102', 'flowers', 'Stanford', 'Cars', 'and', 'iNaturalist1819', 'This', 'paper', 'is', 'organized', 'as', 'follows', 'we', 'review', 'related', 'works', 'in', 'Section', '2', 'and', 'focus', 'on', 'transformers', 'for', 'image', 'classification', 'in', 'Section', '3', 'We', 'introduce', 'our', 'distillation', 'strategy', 'for', 'transformers', 'in', 'Section', '4', 'The', 'experimental', 'section', '5', 'provides', 'analysis', 'and', 'comparisons', 'against', 'both', 'convnets', 'and', 'recent', 'transformers', 'as', 'well', 'as', 'a', 'comparative', 'evaluation', 'of', 'our', 'transformerspecific', 'distillation', 'Section', '6', 'details', 'our', 'training', 'scheme', 'It', 'includes', 'an', 'extensive', 'ablation', 'of', 'our', 'dataefficient', 'training', 'choices', 'which', 'gives', 'some', 'insight', 'on', 'the', 'key', 'ingredients', 'involved', 'in', 'DeiT', 'We', 'conclude', 'in', 'Section', '7', '']\n"
     ]
    }
   ],
   "source": [
    "print(c_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6e50c0f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  3  3  1  2 15  4  1  1 13  2 10  2  6  3 12  5  1  1  1  5 10  3  1\n",
      " 11  1 16  2  2  2 19  4  1  1  1  2  1  3  3  2  6  1  5  1  6  7  5  1\n",
      " 10  2  1  3  1 12  1  2  2  2  4  1  7  1  3  1  1  2  1  1  2  3  2  3\n",
      "  2  1  4  1  4  2  3  1  9  3  1  7 23  1  1  5  3  2  3  2  2  1  4  2\n",
      "  3  1  1  2  4  5  5  2  1  1  1  1 16  1  1  1  9  1  6  2  1  1  3  2\n",
      "  1  1  1  1  1  1  1  2  1  1  2  1  1  1  1  1  2  2  2  1  2  1  1  1\n",
      "  1  1  1  1  2  1  1  1  2  1  1  1  3  1  3  1  2  1  3  1  1  2  2  2\n",
      "  2  3  2  1  1  6  1  1  1  1  1  1  4  1  1  2  1  1  1  1  1  1  1  1\n",
      "  1  1  2  1  1  2  2  2  1  1  1  2  2  2  1  2  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  2  1  1  1  1  1  1  1  3  1  1  1  1  2  1  2  1  2  1\n",
      "  1  1  2  1  1  1  1  2  1  1  1  1  1  1  1  1  1  1  2  1  2  1  1  1\n",
      "  1  2  1  1  2  1  1  1  1  1  1  1  1  1  2  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  5  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(symbol)):\n",
    "    count[i] += c_list.count(symbol[i])\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cfa8b58b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00292398 0.00877193 0.00877193 0.00292398 0.00584795 0.04385965\n",
      " 0.01169591 0.00292398 0.00292398 0.0380117  0.00584795 0.02923977\n",
      " 0.00584795 0.01754386 0.00877193 0.03508772 0.01461988 0.00292398\n",
      " 0.00292398 0.00292398 0.01461988 0.02923977 0.00877193 0.00292398\n",
      " 0.03216374 0.00292398 0.04678363 0.00584795 0.00584795 0.00584795\n",
      " 0.05555556 0.01169591 0.00292398 0.00292398 0.00292398 0.00584795\n",
      " 0.00292398 0.00877193 0.00877193 0.00584795 0.01754386 0.00292398\n",
      " 0.01461988 0.00292398 0.01754386 0.02046784 0.01461988 0.00292398\n",
      " 0.02923977 0.00584795 0.00292398 0.00877193 0.00292398 0.03508772\n",
      " 0.00292398 0.00584795 0.00584795 0.00584795 0.01169591 0.00292398\n",
      " 0.02046784 0.00292398 0.00877193 0.00292398 0.00292398 0.00584795\n",
      " 0.00292398 0.00292398 0.00584795 0.00877193 0.00584795 0.00877193\n",
      " 0.00584795 0.00292398 0.01169591 0.00292398 0.01169591 0.00584795\n",
      " 0.00877193 0.00292398 0.02631579 0.00877193 0.00292398 0.02046784\n",
      " 0.06725146 0.00292398 0.00292398 0.01461988 0.00877193 0.00584795\n",
      " 0.00877193 0.00584795 0.00584795 0.00292398 0.01169591 0.00584795\n",
      " 0.00877193 0.00292398 0.00292398 0.00584795 0.01169591 0.01461988\n",
      " 0.01461988 0.00584795 0.00292398 0.00292398 0.00292398 0.00292398\n",
      " 0.04678363 0.00292398 0.00292398 0.00292398 0.02631579 0.00292398\n",
      " 0.01754386 0.00584795 0.00292398 0.00292398 0.00877193 0.00584795\n",
      " 0.00292398 0.00292398 0.00292398 0.00292398 0.00292398 0.00292398\n",
      " 0.00292398 0.00584795 0.00292398 0.00292398 0.00584795 0.00292398\n",
      " 0.00292398 0.00292398 0.00292398 0.00292398 0.00584795 0.00584795\n",
      " 0.00584795 0.00292398 0.00584795 0.00292398 0.00292398 0.00292398\n",
      " 0.00292398 0.00292398 0.00292398 0.00292398 0.00584795 0.00292398\n",
      " 0.00292398 0.00292398 0.00584795 0.00292398 0.00292398 0.00292398\n",
      " 0.00877193 0.00292398 0.00877193 0.00292398 0.00584795 0.00292398\n",
      " 0.00877193 0.00292398 0.00292398 0.00584795 0.00584795 0.00584795\n",
      " 0.00584795 0.00877193 0.00584795 0.00292398 0.00292398 0.01754386\n",
      " 0.00292398 0.00292398 0.00292398 0.00292398 0.00292398 0.00292398\n",
      " 0.01169591 0.00292398 0.00292398 0.00584795 0.00292398 0.00292398\n",
      " 0.00292398 0.00292398 0.00292398 0.00292398 0.00292398 0.00292398\n",
      " 0.00292398 0.00292398 0.00584795 0.00292398 0.00292398 0.00584795\n",
      " 0.00584795 0.00584795 0.00292398 0.00292398 0.00292398 0.00584795\n",
      " 0.00584795 0.00584795 0.00292398 0.00584795 0.00292398 0.00292398\n",
      " 0.00292398 0.00292398 0.00292398 0.00292398 0.00292398 0.00292398\n",
      " 0.00292398 0.00292398 0.00292398 0.00292398 0.00292398 0.00584795\n",
      " 0.00292398 0.00292398 0.00292398 0.00292398 0.00292398 0.00292398\n",
      " 0.00292398 0.00877193 0.00292398 0.00292398 0.00292398 0.00292398\n",
      " 0.00584795 0.00292398 0.00584795 0.00292398 0.00584795 0.00292398\n",
      " 0.00292398 0.00292398 0.00584795 0.00292398 0.00292398 0.00292398\n",
      " 0.00292398 0.00584795 0.00292398 0.00292398 0.00292398 0.00292398\n",
      " 0.00292398 0.00292398 0.00292398 0.00292398 0.00292398 0.00292398\n",
      " 0.00584795 0.00292398 0.00584795 0.00292398 0.00292398 0.00292398\n",
      " 0.00292398 0.00584795 0.00292398 0.00292398 0.00584795 0.00292398\n",
      " 0.00292398 0.00292398 0.00292398 0.00292398 0.00292398 0.00292398\n",
      " 0.00292398 0.00292398 0.00584795 0.00292398 0.00292398 0.00292398\n",
      " 0.00292398 0.00292398 0.00292398 0.00292398 0.00292398 0.00292398\n",
      " 0.00292398 0.00292398 0.00292398 0.00292398 0.00584795 0.00292398\n",
      " 0.00292398 0.00292398 0.00292398 0.00292398 0.00292398 0.00292398\n",
      " 0.00292398 0.00292398 0.00292398 0.00292398 0.00292398 0.00292398\n",
      " 0.00292398 0.00292398 0.00292398 0.00292398 0.00292398 0.00292398\n",
      " 0.00292398 0.00292398 0.00292398 0.00292398 0.00292398 0.00292398\n",
      " 0.00292398 0.00292398 0.00292398 0.00292398 0.01461988 0.00292398\n",
      " 0.00292398 0.00292398 0.00292398 0.00292398 0.00292398 0.00292398\n",
      " 0.00292398 0.00292398 0.00292398 0.00292398 0.00292398 0.00292398\n",
      " 0.00292398 0.00292398 0.00292398 0.00292398 0.00292398 0.00292398]\n"
     ]
    }
   ],
   "source": [
    "prob = count / len(count)\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7e308c01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Recently', 1), ('neural', 3), ('networks', 3), ('purely', 1), ('based', 2), ('on', 15), ('attention', 4), ('were', 1), ('shown', 1), ('to', 13), ('address', 2), ('image', 10), ('understanding', 2), ('tasks', 6), ('such', 3), ('as', 12), ('classification', 5), ('These', 1), ('high', 1), ('performing', 1), ('vision', 5), ('transformers', 10), ('are', 3), ('pretrained', 1), ('with', 11), ('hundreds', 1), ('of', 16), ('millions', 2), ('images', 2), ('using', 2), ('a', 19), ('large', 4), ('infrastructure', 1), ('thereby', 1), ('limiting', 1), ('their', 2), ('adoption', 1), ('In', 3), ('this', 3), ('work', 2), ('we', 6), ('produce', 1), ('competitive', 5), ('convolutionfree', 1), ('by', 6), ('training', 7), ('Imagenet', 5), ('only', 1), ('We', 10), ('train', 2), ('them', 1), ('single', 3), ('computer', 1), ('in', 12), ('less', 1), ('than', 2), ('3', 2), ('days', 2), ('Our', 4), ('reference', 1), ('transformer', 7), ('86M', 1), ('parameters', 3), ('achieves', 1), ('top1', 1), ('accuracy', 2), ('831', 1), ('singlecrop', 1), ('ImageNet', 2), ('no', 3), ('external', 2), ('data', 3), ('More', 2), ('importantly', 1), ('introduce', 4), ('teacherstudent', 1), ('strategy', 4), ('specific', 2), ('It', 3), ('relies', 1), ('distillation', 9), ('token', 3), ('ensuring', 1), ('that', 7), ('the', 23), ('student', 1), ('learns', 1), ('from', 5), ('teacher', 3), ('through', 2), ('show', 3), ('interest', 2), ('tokenbased', 2), ('especially', 1), ('when', 4), ('convnet', 2), ('This', 3), ('leads', 1), ('us', 1), ('report', 2), ('results', 4), ('convnets', 5), ('for', 5), ('both', 2), ('where', 1), ('obtain', 1), ('up', 1), ('852', 1), ('and', 16), ('transferring', 1), ('other', 1), ('share', 1), ('our', 9), ('code', 1), ('models', 6), ('1', 2), ('Introduction', 1), ('Convolutional', 1), ('have', 3), ('been', 2), ('main', 1), ('design', 1), ('paradigm', 1), ('initially', 1), ('demonstrated', 1), ('One', 1), ('ingredient', 1), ('success', 2), ('was', 1), ('availability', 1), ('set', 2), ('namely', 1), ('13', 1), ('42', 1), ('Motivated', 1), ('attentionbased', 1), ('Natural', 2), ('Language', 2), ('Processing', 2), ('14', 1), ('52', 2), ('there', 1), ('has', 1), ('increasing', 1), ('architectures', 1), ('leveraging', 1), ('mechanisms', 1), ('within', 1), ('2', 2), ('34', 1), ('61', 1), ('recently', 1), ('several', 2), ('researchers', 1), ('proposed', 1), ('hybrid', 1), ('architecture', 3), ('transplanting', 1), ('ingredients', 3), ('solve', 1), ('6', 2), ('43', 1), ('The', 3), ('ViT', 1), ('introduced', 1), ('Dosovitskiy', 2), ('et', 2), ('al', 2), ('15', 2), ('is', 3), ('an', 2), ('directly', 1), ('inherited', 1), ('', 6), ('but', 1), ('applied', 1), ('raw', 1), ('patches', 1), ('input', 1), ('Their', 1), ('paper', 4), ('presented', 1), ('excellent', 1), ('trained', 2), ('private', 1), ('labelled', 1), ('dataset', 1), ('JFT300M', 1), ('46', 1), ('300', 1), ('concluded', 1), ('do', 1), ('not', 1), ('generalize', 1), ('well', 2), ('insufficient', 1), ('amounts', 1), ('these', 2), ('involved', 2), ('extensive', 2), ('computing', 1), ('resources', 1), ('8GPU', 1), ('node', 2), ('two', 2), ('three', 2), ('53', 1), ('hours', 2), ('pretraining', 1), ('optionally', 1), ('20', 1), ('finetuning', 1), ('having', 1), ('similar', 1), ('number', 1), ('efficiency', 1), ('uses', 1), ('sole', 1), ('build', 1), ('upon', 1), ('visual', 1), ('improvements', 2), ('included', 1), ('timm', 1), ('library', 1), ('55', 1), ('With', 1), ('Dataefficient', 1), ('Transformers', 1), ('DeiT', 3), ('over', 1), ('previous', 1), ('see', 1), ('Figure', 1), ('ablation', 2), ('study', 1), ('details', 2), ('hyperparameters', 1), ('key', 2), ('successful', 1), ('repeated', 1), ('augmentation', 1), ('another', 2), ('question', 1), ('how', 1), ('distill', 1), ('denoted', 1), ('it', 2), ('advantageously', 1), ('replaces', 1), ('usual', 1), ('summary', 1), ('makes', 1), ('following', 1), ('contributions', 1), ('contains', 1), ('convolutional', 1), ('layer', 1), ('can', 2), ('achieve', 1), ('against', 2), ('state', 1), ('art', 1), ('They', 1), ('learned', 1), ('4', 2), ('GPUs', 1), ('days1', 1), ('new', 2), ('DeiTS', 1), ('DeiTTi', 1), ('fewer', 1), ('be', 1), ('seen', 1), ('counterpart', 1), ('ResNet50', 1), ('ResNet18', 1), ('procedure', 1), ('which', 2), ('plays', 1), ('same', 1), ('role', 1), ('class', 1), ('except', 1), ('aims', 1), ('at', 1), ('reproducing', 1), ('label', 1), ('estimated', 1), ('Both', 1), ('tokens', 1), ('interact', 1), ('transformerspecific', 2), ('outperforms', 1), ('vanilla', 1), ('significant', 1), ('margin', 1), ('Interestingly', 1), ('learn', 1), ('more', 1), ('comparable', 1), ('performance', 1), ('prelearned', 1), ('transferred', 1), ('different', 1), ('downstream', 1), ('finegrained', 1), ('popular', 1), ('public', 1), ('benchmarks', 1), ('CIFAR10', 1), ('CIFAR100', 1), ('Oxford102', 1), ('flowers', 1), ('Stanford', 1), ('Cars', 1), ('iNaturalist1819', 1), ('organized', 1), ('follows', 1), ('review', 1), ('related', 1), ('works', 1), ('Section', 5), ('focus', 1), ('experimental', 1), ('section', 1), ('5', 1), ('provides', 1), ('analysis', 1), ('comparisons', 1), ('recent', 1), ('comparative', 1), ('evaluation', 1), ('scheme', 1), ('includes', 1), ('dataefficient', 1), ('choices', 1), ('gives', 1), ('some', 1), ('insight', 1), ('conclude', 1), ('7', 1)]\n"
     ]
    }
   ],
   "source": [
    "sym_cnt = []\n",
    "for pair in zip(symbol,count):\n",
    "    sym_cnt.append(pair)\n",
    "print(sym_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "76a53ad5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Recently': '001011110', 'neural': '10100110', 'networks': '10100101', 'purely': '1111010000', 'based': '111110101', 'on': '110001', 'attention': '0101100', 'were': '001101011', 'shown': '010100110', 'to': '101111', 'address': '111110100', 'image': '011110', 'understanding': '110110000', 'tasks': '1010100', 'such': '10100011', 'as': '100110', 'classification': '1000010', 'These': '1110011100', 'high': '010100101', 'performing': '000000111', 'vision': '0111001', 'transformers': '011101', 'are': '10101111', 'pretrained': '010001101', 'with': '100010', 'hundreds': '1101100110', 'of': '110011', 'millions': '111011110', 'images': '110010100', 'using': '00100110', 'a': '01101', 'large': '0000011', 'infrastructure': '1100101011', 'thereby': '010000101', 'limiting': '001101010', 'their': '01000111', 'adoption': '000001011', 'In': '10100001', 'this': '10101100', 'work': '01000000', 'we': '1011000', 'produce': '1110010011', 'competitive': '0110010', 'convolutionfree': '1111101101', 'by': '1011001', 'training': '1011101', 'Imagenet': '0111000', 'only': '1110000101', 'We': '011111', 'train': '00001000', 'them': '1111000111', 'single': '10101101', 'computer': '1111010111', 'in': '100011', 'less': '001110100', 'than': '00011110', '3': '111101001', 'days': '111000001', 'Our': '0000110', 'reference': '001001001', 'transformer': '1100001', '86M': '001001111', 'parameters': '10100100', 'achieves': '010101101', 'top1': '1111011010', 'accuracy': '01010000', '831': '1100101010', 'singlecrop': '010000111', 'ImageNet': '00100001', 'no': '10110110', 'external': '00110001', 'data': '10111001', 'More': '00101100', 'importantly': '000100001', 'introduce': '11101110', 'teacherstudent': '000010010', 'strategy': '0010101', 'specific': '00101000', 'It': '10111000', 'relies': '1111101110', 'distillation': '010111', 'token': '10011111', 'ensuring': '000000110', 'that': '1100000', 'the': '10010', 'student': '001000101', 'learns': '1101101011', 'from': '0110011', 'teacher': '10110111', 'through': '111110000', 'show': '10100111', 'interest': '111111101', 'tokenbased': '00110010', 'especially': '1111111000', 'when': '0000101', 'convnet': '00101101', 'This': '10101110', 'leads': '010001100', 'us': '010001010', 'report': '00010010', 'results': '0011100', 'convnets': '1000001', 'for': '1000000', 'both': '111100111', 'where': '1111000010', 'obtain': '1111001101', 'up': '1111010110', '852': '1110100001', 'and': '110101', 'transferring': '001110110', 'other': '1101100011', 'share': '010011110', 'our': '011000', 'code': '000110001', 'models': '1001110', '1': '111000111', 'Introduction': '010010011', 'Convolutional': '1110101010', 'have': '10000110', 'been': '00011001', 'main': '001001000', 'design': '000101000', 'paradigm': '001101111', 'initially': '1100101111', 'demonstrated': '001001110', 'One': '010010000', 'ingredient': '1010101111', 'success': '111101110', 'was': '1110111111', 'availability': '000011101', 'set': '111101100', 'namely': '1101110001', '13': '1110000000', '42': '1111101111', 'Motivated': '1110101111', 'attentionbased': '010000010', 'Natural': '00000000', 'Language': '111011000', 'Processing': '110110111', '14': '001000111', '52': '00111100', 'there': '001111010', 'has': '1101110110', 'increasing': '1101001010', 'architectures': '1101110101', 'leveraging': '001100110', 'mechanisms': '000001001', 'within': '1111000100', '2': '111111011', '34': '1110011000', '61': '1111011111', 'recently': '1111001000', 'several': '111010001', 'researchers': '1110100101', 'proposed': '1101111011', 'hybrid': '1101100101', 'architecture': '10100000', 'transplanting': '010100100', 'ingredients': '10000111', 'solve': '1110001000', '6': '111101010', '43': '010010111', 'The': '10011110', 'ViT': '1110101011', 'introduced': '1110110100', 'Dosovitskiy': '111001011', 'et': '110010010', 'al': '00011100', '15': '00101110', 'is': '10100010', 'an': '110110110', 'directly': '001000110', 'inherited': '001000100', '': '1011010', 'but': '1101101010', 'applied': '001000000', 'raw': '001111011', 'patches': '1111101100', 'input': '1101001101', 'Their': '1101001100', 'paper': '11010001', 'presented': '1101001111', 'excellent': '1101001110', 'trained': '111111111', 'private': '1101001001', 'labelled': '1111100011', 'dataset': '1111100010', 'JFT300M': '1111111001', '46': '1111111101', '300': '1111111100', 'concluded': '001100111', 'do': '001100001', 'not': '001100000', 'generalize': '1110000100', 'well': '00010001', 'insufficient': '000100000', 'amounts': '010001011', 'these': '01000100', 'involved': '111001101', 'extensive': '111100101', 'computing': '1110011101', 'resources': '1110011111', '8GPU': '1110011110', 'node': '111111010', 'two': '111111001', 'three': '111111000', '53': '1111000110', 'hours': '111010011', 'pretraining': '000010011', 'optionally': '1111000011', '20': '010011101', 'finetuning': '1110100100', 'having': '1111001100', 'similar': '1101100111', 'number': '010011001', 'efficiency': '1101100100', 'uses': '1110100000', 'sole': '1111000001', 'build': '1111000000', 'upon': '001110111', 'visual': '001110101', 'improvements': '01001101', 'included': '1101100010', 'timm': '010011111', 'library': '010100011', '55': '010100010', 'With': '010010101', 'Dataefficient': '010010110', 'Transformers': '000110000', 'DeiT': '10101010', 'over': '000111111', 'previous': '000111110', 'see': '001010011', 'Figure': '010010010', 'ablation': '111000101', 'study': '1101000001', 'details': '00011011', 'hyperparameters': '1101000000', 'key': '111000011', 'successful': '1110101001', 'repeated': '1110101000', 'augmentation': '1110010001', 'another': '111001010', 'question': '1110010000', 'how': '1100101101', 'distill': '1100101100', 'denoted': '001101110', 'it': '110010000', 'advantageously': '1100101110', 'replaces': '1110010010', 'usual': '001011111', 'summary': '010010001', 'makes': '001001011', 'following': '001001010', 'contributions': '1010101110', 'contains': '010101111', 'convolutional': '101010110', 'layer': '1110111110', 'can': '00001111', 'achieve': '1111011011', 'against': '01010101', 'state': '000101101', 'art': '000101100', 'They': '010101000', 'learned': '1110000001', '4': '111110010', 'GPUs': '1101000011', 'days1': '1101000010', 'new': '111110011', 'DeiTS': '1110110111', 'DeiTTi': '1110101110', 'fewer': '1110110101', 'be': '000101111', 'seen': '000101110', 'counterpart': '000110101', 'ResNet50': '000110100', 'ResNet18': '1110101101', 'procedure': '1110101100', 'which': '00110100', 'plays': '1101101000', 'same': '001101101', 'role': '001101100', 'class': '000101011', 'except': '001000001', 'aims': '000101010', 'at': '1101110111', 'reproducing': '1100100111', 'label': '1101001011', 'estimated': '1100100110', 'Both': '1101001000', 'tokens': '1101110100', 'interact': '1101111111', 'transformerspecific': '00010011', 'outperforms': '1101111110', 'vanilla': '000001000', 'significant': '1111000101', 'margin': '1101111101', 'Interestingly': '1101111100', 'learn': '1100100011', 'more': '1110011001', 'comparable': '1100100010', 'performance': '000001010', 'prelearned': '1111011110', 'transferred': '1111001001', 'different': '010101110', 'downstream': '010101100', 'finegrained': '000111011', 'popular': '010011100', 'public': '000111010', 'benchmarks': '000011100', 'CIFAR10': '1101111010', 'CIFAR100': '010011000', 'Oxford102': '1101111001', 'flowers': '1101111000', 'Stanford': '010100111', 'Cars': '1111010001', 'iNaturalist1819': '1101110011', 'organized': '1101110010', 'follows': '010101001', 'review': '1110001001', 'related': '1101110000', 'works': '000000011', 'Section': '0101101', 'focus': '010010100', 'experimental': '000000010', 'section': '001111101', '5': '001111100', 'provides': '001010010', 'analysis': '1110110110', 'comparisons': '1110001101', 'recent': '010000011', 'comparative': '1110001100', 'evaluation': '010000110', 'scheme': '000000101', 'includes': '000000100', 'dataefficient': '000101001', 'choices': '010000100', 'gives': '001111111', 'some': '001111110', 'insight': '1110110011', 'conclude': '1110110010', '7': '1101101001'}\n"
     ]
    }
   ],
   "source": [
    "huff_code = huffman.codebook(sym_cnt)\n",
    "print(huff_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0b772acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average code length: 16.356725146198954\n"
     ]
    }
   ],
   "source": [
    "avglen = 0\n",
    "for i in range(len(prob)):\n",
    "    avglen += prob[i] * len(huff_code[symbol[i]])\n",
    "print(f\"average code length: {avglen}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "04ae7ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_str = ' '.join(c_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1b675bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Recently': 1, 'neural': 3, 'networks': 3, 'purely': 1, 'based': 2, 'on': 15, 'attention': 4, 'were': 1, 'shown': 1, 'to': 13, 'address': 2, 'image': 10, 'understanding': 2, 'tasks': 6, 'such': 3, 'as': 12, 'classification': 5, 'These': 1, 'high': 1, 'performing': 1, 'vision': 5, 'transformers': 10, 'are': 3, 'pretrained': 1, 'with': 11, 'hundreds': 1, 'of': 16, 'millions': 2, 'images': 2, 'using': 2, 'a': 19, 'large': 4, 'infrastructure': 1, 'thereby': 1, 'limiting': 1, 'their': 2, 'adoption': 1, 'In': 3, 'this': 3, 'work': 2, 'we': 6, 'produce': 1, 'competitive': 5, 'convolutionfree': 1, 'by': 6, 'training': 7, 'Imagenet': 5, 'only': 1, 'We': 10, 'train': 2, 'them': 1, 'single': 3, 'computer': 1, 'in': 12, 'less': 1, 'than': 2, '3': 2, 'days': 2, 'Our': 4, 'reference': 1, 'transformer': 7, '86M': 1, 'parameters': 3, 'achieves': 1, 'top1': 1, 'accuracy': 2, '831': 1, 'singlecrop': 1, 'ImageNet': 2, 'no': 3, 'external': 2, 'data': 3, 'More': 2, 'importantly': 1, 'introduce': 4, 'teacherstudent': 1, 'strategy': 4, 'specific': 2, 'It': 3, 'relies': 1, 'distillation': 9, 'token': 3, 'ensuring': 1, 'that': 7, 'the': 23, 'student': 1, 'learns': 1, 'from': 5, 'teacher': 3, 'through': 2, 'show': 3, 'interest': 2, 'tokenbased': 2, 'especially': 1, 'when': 4, 'convnet': 2, 'This': 3, 'leads': 1, 'us': 1, 'report': 2, 'results': 4, 'convnets': 5, 'for': 5, 'both': 2, 'where': 1, 'obtain': 1, 'up': 1, '852': 1, 'and': 16, 'transferring': 1, 'other': 1, 'share': 1, 'our': 9, 'code': 1, 'models': 6, '1': 2, 'Introduction': 1, 'Convolutional': 1, 'have': 3, 'been': 2, 'main': 1, 'design': 1, 'paradigm': 1, 'initially': 1, 'demonstrated': 1, 'One': 1, 'ingredient': 1, 'success': 2, 'was': 1, 'availability': 1, 'set': 2, 'namely': 1, '13': 1, '42': 1, 'Motivated': 1, 'attentionbased': 1, 'Natural': 2, 'Language': 2, 'Processing': 2, '14': 1, '52': 2, 'there': 1, 'has': 1, 'increasing': 1, 'architectures': 1, 'leveraging': 1, 'mechanisms': 1, 'within': 1, '2': 2, '34': 1, '61': 1, 'recently': 1, 'several': 2, 'researchers': 1, 'proposed': 1, 'hybrid': 1, 'architecture': 3, 'transplanting': 1, 'ingredients': 3, 'solve': 1, '6': 2, '43': 1, 'The': 3, 'ViT': 1, 'introduced': 1, 'Dosovitskiy': 2, 'et': 2, 'al': 2, '15': 2, 'is': 3, 'an': 2, 'directly': 1, 'inherited': 1, '': 6, 'but': 1, 'applied': 1, 'raw': 1, 'patches': 1, 'input': 1, 'Their': 1, 'paper': 4, 'presented': 1, 'excellent': 1, 'trained': 2, 'private': 1, 'labelled': 1, 'dataset': 1, 'JFT300M': 1, '46': 1, '300': 1, 'concluded': 1, 'do': 1, 'not': 1, 'generalize': 1, 'well': 2, 'insufficient': 1, 'amounts': 1, 'these': 2, 'involved': 2, 'extensive': 2, 'computing': 1, 'resources': 1, '8GPU': 1, 'node': 2, 'two': 2, 'three': 2, '53': 1, 'hours': 2, 'pretraining': 1, 'optionally': 1, '20': 1, 'finetuning': 1, 'having': 1, 'similar': 1, 'number': 1, 'efficiency': 1, 'uses': 1, 'sole': 1, 'build': 1, 'upon': 1, 'visual': 1, 'improvements': 2, 'included': 1, 'timm': 1, 'library': 1, '55': 1, 'With': 1, 'Dataefficient': 1, 'Transformers': 1, 'DeiT': 3, 'over': 1, 'previous': 1, 'see': 1, 'Figure': 1, 'ablation': 2, 'study': 1, 'details': 2, 'hyperparameters': 1, 'key': 2, 'successful': 1, 'repeated': 1, 'augmentation': 1, 'another': 2, 'question': 1, 'how': 1, 'distill': 1, 'denoted': 1, 'it': 2, 'advantageously': 1, 'replaces': 1, 'usual': 1, 'summary': 1, 'makes': 1, 'following': 1, 'contributions': 1, 'contains': 1, 'convolutional': 1, 'layer': 1, 'can': 2, 'achieve': 1, 'against': 2, 'state': 1, 'art': 1, 'They': 1, 'learned': 1, '4': 2, 'GPUs': 1, 'days1': 1, 'new': 2, 'DeiTS': 1, 'DeiTTi': 1, 'fewer': 1, 'be': 1, 'seen': 1, 'counterpart': 1, 'ResNet50': 1, 'ResNet18': 1, 'procedure': 1, 'which': 2, 'plays': 1, 'same': 1, 'role': 1, 'class': 1, 'except': 1, 'aims': 1, 'at': 1, 'reproducing': 1, 'label': 1, 'estimated': 1, 'Both': 1, 'tokens': 1, 'interact': 1, 'transformerspecific': 2, 'outperforms': 1, 'vanilla': 1, 'significant': 1, 'margin': 1, 'Interestingly': 1, 'learn': 1, 'more': 1, 'comparable': 1, 'performance': 1, 'prelearned': 1, 'transferred': 1, 'different': 1, 'downstream': 1, 'finegrained': 1, 'popular': 1, 'public': 1, 'benchmarks': 1, 'CIFAR10': 1, 'CIFAR100': 1, 'Oxford102': 1, 'flowers': 1, 'Stanford': 1, 'Cars': 1, 'iNaturalist1819': 1, 'organized': 1, 'follows': 1, 'review': 1, 'related': 1, 'works': 1, 'Section': 5, 'focus': 1, 'experimental': 1, 'section': 1, '5': 1, 'provides': 1, 'analysis': 1, 'comparisons': 1, 'recent': 1, 'comparative': 1, 'evaluation': 1, 'scheme': 1, 'includes': 1, 'dataefficient': 1, 'choices': 1, 'gives': 1, 'some': 1, 'insight': 1, 'conclude': 1, '7': 1}\n",
      "No\n"
     ]
    }
   ],
   "source": [
    "symcnt_dict = dict(zip(symbol,count))\n",
    "print(symcnt_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6937117d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'>\n",
      "2547\n",
      "4688\n"
     ]
    }
   ],
   "source": [
    "#codec = HuffmanCodec.from_frequencies(symcnt_dict)\n",
    "codec = HuffmanCodec.from_data(text_str)\n",
    "encoded = codec.encode(text_str)\n",
    "print(type(encoded))\n",
    "print(len(encoded))\n",
    "print(len(codec.decode(encoded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "93fb89d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text bit: 37504 bit\n",
      "huffman code bit: 20376 bit\n",
      "Compression rate: 1.8405967805261092\n"
     ]
    }
   ],
   "source": [
    "text_bit = len(text_str) * 8\n",
    "huff_bit = len(encoded) * 8\n",
    "print(f\"text bit: {text_bit} bit\")\n",
    "huff_len = len(encoded)\n",
    "#for i in range(len(count)):\n",
    "#    huff_len += count[i] * len(huff_code[symbol[i]])\n",
    "print(f\"huffman code bit: {huff_bit} bit\")\n",
    "print(f\"Compression rate: {text_bit / huff_bit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3ce8772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recently neural networks purely based on attention were shown to address image understanding tasks such as image classification These high performing vision transformers are pretrained with hundreds of millions of images using a large infrastructure thereby limiting their adoption In this work we produce competitive convolutionfree transformers by training on Imagenet only We train them on a single computer in less than 3 days Our reference vision transformer 86M parameters achieves top1 accuracy of 831 singlecrop on ImageNet with no external data More importantly we introduce a teacherstudent strategy specific to transformers It relies on a distillation token ensuring that the student learns from the teacher through attention We show the interest of this tokenbased distillation especially when using a convnet as a teacher This leads us to report results competitive with convnets for both Imagenet where we obtain up to 852 accuracy and when transferring to other tasks We share our code and models 1 Introduction Convolutional neural networks have been the main design paradigm for image understanding tasks as initially demonstrated on image classification tasks One of the ingredient to their success was the availability of a large training set namely Imagenet 13 42 Motivated by the success of attentionbased models in Natural Language Processing 14 52 there has been increasing interest in architectures leveraging attention mechanisms within convnets 2 34 61 More recently several researchers have proposed hybrid architecture transplanting transformer ingredients to convnets to solve vision tasks 6 43 The vision transformer ViT introduced by Dosovitskiy et al 15 is an architecture directly inherited from Natural Language Processing 52  but applied to image classification with raw image patches as input Their paper presented excellent results with transformers trained with a large private labelled image dataset JFT300M 46 300 millions images The paper concluded that transformers do not generalize well when trained on insufficient amounts of data and the training of these models involved extensive computing resources In this paper we train a vision transformer on a single 8GPU node in two to three days 53 hours of pretraining and optionally 20 hours of finetuning that is competitive with convnets having a similar number of parameters and efficiency It uses Imagenet as the sole training set We build upon the visual transformer architecture from Dosovitskiy et al 15 and improvements included in the timm library 55 With our Dataefficient image Transformers DeiT we report large improvements over previous results see Figure 1 Our ablation study details the hyperparameters and key ingredients for a successful training such as repeated augmentation We address another question how to distill these models We introduce a tokenbased strategy specific to transformers and denoted by DeiT and show that it advantageously replaces the usual distillation In summary our work makes the following contributions  We show that our neural networks that contains no convolutional layer can achieve competitive results against the state of the art on ImageNet with no external data They are learned on a single node with 4 GPUs in three days1 Our two new models DeiTS and DeiTTi have fewer parameters and can be seen as the counterpart of ResNet50 and ResNet18  We introduce a new distillation procedure based on a distillation token which plays the same role as the class token except that it aims at reproducing the label estimated by the teacher Both tokens interact in the transformer through attention This transformerspecific strategy outperforms vanilla distillation by a significant margin  Interestingly with our distillation image transformers learn more from a convnet than from another transformer with comparable performance  Our models prelearned on Imagenet are competitive when transferred to different downstream tasks such as finegrained classification on several popular public benchmarks CIFAR10 CIFAR100 Oxford102 flowers Stanford Cars and iNaturalist1819 This paper is organized as follows we review related works in Section 2 and focus on transformers for image classification in Section 3 We introduce our distillation strategy for transformers in Section 4 The experimental section 5 provides analysis and comparisons against both convnets and recent transformers as well as a comparative evaluation of our transformerspecific distillation Section 6 details our training scheme It includes an extensive ablation of our dataefficient training choices which gives some insight on the key ingredients involved in DeiT We conclude in Section 7 \n"
     ]
    }
   ],
   "source": [
    "print(text_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93924904",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
